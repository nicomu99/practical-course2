{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this notebook, the data is prepared for training. It contains the following steps:\n",
    "\n",
    "- Remove duplicate lines\n",
    "- Remove personas with equal performance."
   ],
   "id": "799d676a0f136f3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re"
   ],
   "id": "6abadd7529d6f393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = pd.read_parquet('data/agent_compt.parquet')",
   "id": "58019c1a93cf850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Check inputs with 16 personas\n",
    "\n",
    "We assume that these are always duplicates, but further analysis has shown that this is not true."
   ],
   "id": "7cdb8e2c44691d5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "counts = data['input'].map(data['input'].value_counts())\n",
    "filtered_df = data[counts == 16]\n",
    "filtered_df"
   ],
   "id": "c8d5904b0c99b965",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Even though some agents seem to be duplicates, they are just very similar. Some seem to be very similar, yet not the same. So inputs with 16 agents seem to be part of the dataset.",
   "id": "a30475d13d600414"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Remove trailing line breaks\n",
    "\n",
    "Some rows contain agent strings that only differ by a double line break. We remove the double line breaks so the duplicates get identified correctly."
   ],
   "id": "fa5504912badb630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data['agent'] = data['agent'].str.replace('\\\\n\\\\n', '\\\\n', regex=True)\n",
    "data['agent'] = data['agent'].str.rstrip()"
   ],
   "id": "fd1b5b7c40329f21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Swap 'None' with empty string",
   "id": "69aec1b67a5d8640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = data.replace({None: ''})",
   "id": "ddc987a7daf7dc3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get rid of chat markers",
   "id": "1a34734ad5104859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data['input'] = data['input'].apply(lambda x: x.replace('<end_of_turn>\\n<start_of_turn>model\\n', '').strip())",
   "id": "886c58e7ff33988b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Remove any duplicates",
   "id": "a8df612e135f7960"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data['agent'] = data['agent'].str.split('\\n\\n').str[0]\n",
    "def clean_agent(text):\n",
    "    # Truncate after double new line\n",
    "    text = text.split('\\n\\n')[0]\n",
    "\n",
    "    # Remove Markdown text styles\n",
    "    text = re.sub(r'(\\*\\*|\\*|__|_)(.*?)\\1', r'\\2', text)\n",
    "\n",
    "    # Remove multiple whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the function to the 'agent' column\n",
    "data['agent'] = data['agent'].apply(clean_agent)\n",
    "data_unique = data.drop_duplicates(subset=['input', 'agent'], keep='first')"
   ],
   "id": "5a527cd19ad4eb4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check outcome",
   "id": "50b7b049d96e638a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_unique['input'].value_counts().unique()",
   "id": "363b998b8712310a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_unique[data_unique['input'] == \"A 14-year-old girl is brought to the physician after her mother learned that she began having sexual intercourse with various partners 1 month ago. She does not use condoms or other contraception. The mother is concerned about her behavior. The patient's parents separated 3 months ago. She had been an honor student and excelled in sports and leadership positions at school before the separation. Since the separation, however, she has become sullen, defiant, and rebellious. She has begun smoking cigarettes, disobeying her curfew, and being truant from school. This patient is most likely using which of the following defense mechanisms?\\n(A) Acting out\\n(B) Intellectualization\\n(C) Projection\\n(D) Regression\\n(E) Displacement\\n(F) Rationalization\\n(G) Denial\\n(H) Repression\\n(I) Sublimation\\n(J) Reaction formation\"]",
   "id": "f1deced1c10ca06f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are still inputs with more than 8 different personas.",
   "id": "c3f3ff57b377162b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f'Deleted rows: {len(data.index) - len(data_unique.index)}')",
   "id": "cd5fbdd117b3e037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check for empty lines",
   "id": "13191e46c3c356be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_unique.info()",
   "id": "92350a22ee83d394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Empty agent column indicates no persona was used. This is not an actual empty line. These will not be deleted. All other columns contain the expected (full) amount of entries.",
   "id": "c35c60575aad7e1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean unusable instances\n",
    "\n",
    "For the training to work, we need some sort of performance hierarchy between the agents. Thus, all instances will be deleted where either all or no agents are able to complete the task. We can not rank these agents since they all perform equally good or bad."
   ],
   "id": "5f4a8f8185ee9bf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group rows by input and keep only if unique values in score is larger than 1 (as else, all would be the same)\n",
    "data_filtered = data_unique[data_unique.groupby('input')['score'].transform(lambda x: x.nunique() > 1)]"
   ],
   "id": "192a539268836fe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Some information about the data",
   "id": "359ef1b21f85245d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f'Deleted rows: {len(data.index) - len(data_filtered.index)}')",
   "id": "661f521cf76de0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f'Remaining rows: {len(data_filtered.index)}')",
   "id": "9bca2bf2ac5b1a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Avg. agents per prompt: {len(data_filtered) / len(data_filtered['input'].unique())}\")",
   "id": "f819a7e229e1403a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Number of winning agents: {len(data_filtered[data_filtered['score'] == 1])}\")",
   "id": "b546e9035277b9b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Number of losing agents: {len(data_filtered[data_filtered['score'] == 0])}\")",
   "id": "b9490966637da6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup data for fine-tuning",
   "id": "94012c5f3df027e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fine_tune_data = data_filtered[data_filtered['score'] == 1]\n",
    "fine_tune_data = fine_tune_data[fine_tune_data['agent'] != '']"
   ],
   "id": "5eac049ea7d89637",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "finetune_task_string = 'Given the following task, print a suitable agent string that will answer it as good as possible: '\n",
    "fine_tune_data['prompt'] = finetune_task_string + fine_tune_data['input'].str.strip() + \"\\n\"\n",
    "fine_tune_data['completion'] = \" \" + fine_tune_data['agent'].str.strip()"
   ],
   "id": "8998f31049dca84c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fine_tune_data = fine_tune_data[['prompt', 'completion']]\n",
    "fine_tune_data"
   ],
   "id": "3f03f56403628607",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fine_tune_data.to_parquet('data/agent_finetune.parquet', index=False)",
   "id": "9a4294457147a8d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup training dataset to allow DPO training\n",
    "\n",
    "In this section, the actual dataset for training, testing and evaluating the model will be constructed. This will be done according to the documentation here: https://huggingface.co/docs/trl/main/en/dataset_formats#preference\n",
    "\n",
    "There are two possible formats standard and conversational, as well as implicit and explicit form. Please refer to the documentation linked above to see examples.\n",
    "\n",
    "In the following code, the dataset is built in the so-called standard format with explicit prompts (which is recommended).\n",
    "\n",
    "Training samples are tuples of $(x, y_w, y_l)$, where $y_w$ is a winning agent prompt string, $y_l$ is a losing agent prompt string and $x$ is the model input."
   ],
   "id": "27384701e1dee4a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = []\n",
    "analysis_data = []\n",
    "count = 0\n",
    "for input_val, group in data_filtered.groupby('input'):\n",
    "    # Group by input and get winning and losing outputs\n",
    "    winning_agents = group[group['score'] == 1]\n",
    "    losing_agents = group[group['score'] == 0]\n",
    "\n",
    "    for idx_winning, winning in winning_agents.iterrows():\n",
    "        for idx_losing, losing in losing_agents.iterrows():\n",
    "            # As a sanity check I want to check that the inputs are actually the same\n",
    "            if not winning['input'] == losing['input']:\n",
    "                raise Exception(\"Illegal combination of winning and losing input\")\n",
    "\n",
    "            # Task string was updated in favor of the new option\n",
    "            model_task = 'Given the following task, print a suitable agent string that will answer it as good as possible: '\n",
    "\n",
    "            # Actual dataset for training the model using conversational, explicit format\n",
    "            train_data.append({\n",
    "                'category': winning['category'],\n",
    "                'orig_prompt': input_val,\n",
    "                'prompt': [{'role': 'user', 'content': f'{model_task} {input_val}'}],\n",
    "                'chosen': [{'role': 'assistant', 'content': winning['agent']}],\n",
    "                'rejected': [{'role': 'assistant', 'content': losing['agent']}]\n",
    "            })\n",
    "\n",
    "            # The list entries of columns prompt and chosen are difficult to analyze\n",
    "            analysis_data.append({\n",
    "                'category': winning['category'],\n",
    "                'orig_prompt': input_val,\n",
    "                'prompt': f'{model_task} {input_val}', # or losing['input'] or winning['input']\n",
    "                'chosen': winning['agent'],\n",
    "                'rejected': losing['agent']\n",
    "            })"
   ],
   "id": "7dcd06923cb27933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f'Training (and test/eval) samples: {len(train_data)}')",
   "id": "c7dbcdc7b81c6259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.to_parquet('data/agent_prompt_cleaned.parquet', index=False)\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_data)\n",
    "analysis_df.to_parquet('data_analysis/agent_prompt_cleaned.parquet', index=False)"
   ],
   "id": "fea4ca4b70e90330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.head()",
   "id": "e23782a7b62d7767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare test set\n",
    "\n",
    "Testing is performed on out-of-training categories with over 1000 inputs in the test set."
   ],
   "id": "e40fa123ec396eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df['category'].value_counts()",
   "id": "88079693304d0c1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_categories = ['biology', 'philosophy', 'psychology',\n",
    "                   'social_iqa', 'arithmetic', 'elementary_math_qa', 'history', 'economics']\n",
    "\n",
    "test_df = train_df[train_df['category'].isin(test_categories)]\n",
    "train_df = train_df[~train_df['category'].isin(test_categories)]\n",
    "\n",
    "test_analysis_df = analysis_df[analysis_df['category'].isin(test_categories)]\n",
    "train_analysis_df = analysis_df[~analysis_df['category'].isin(test_categories)]\n",
    "len(test_df['category'])"
   ],
   "id": "e82e06e9bf36549b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(test_df.orig_prompt.unique())",
   "id": "342b3bdab7966817",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df = test_df.drop(columns=['orig_prompt'])\n",
    "train_df = train_df.drop(columns=['orig_prompt'])"
   ],
   "id": "3ada6131e4c0d417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df.to_parquet('data/agent_test.parquet', index=False)\n",
    "train_df.to_parquet('data/agent_train.parquet', index=False)\n",
    "\n",
    "test_analysis_df.to_parquet('data_analysis/agent_test.parquet', index=False)\n",
    "train_analysis_df.to_parquet('data_analysis/agent_train.parquet', index=False)"
   ],
   "id": "4b727b0c4a8fda5d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
