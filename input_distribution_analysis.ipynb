{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Input Distribution Analysis\n",
    "\n",
    "We have prompt, rejected, chosen. All can be multiple times, only tuples <prompt, rejected, chosen> is \"really\" unique."
   ],
   "id": "9a78cceed79147c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading",
   "id": "d8ea9999ba06622d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "71406e32c942aeb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "whole_df = pd.read_parquet('data_analysis/agent_prompt_cleaned.parquet')\n",
    "train_df = pd.read_parquet('data_analysis/agent_train.parquet')\n",
    "test_df = pd.read_parquet('data_analysis/agent_test.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stripped_train_df = train_df[(train_df['chosen'].str.strip() != '')]\n",
    "stripped_test_df = test_df[(test_df['chosen'].str.strip() != '')]"
   ],
   "id": "8f1efa873b38ccff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot styling",
   "id": "3e9a40c421cddc79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def style_wrapper(\n",
    "        axis, title=None, x_label=None, y_label=None, show_legend=False,\n",
    "        x_start_at_zero=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom styling for matplotlib axis. Removes the upper and right spines, the remaining two\n",
    "    have a thicker line. Adds horizontal grid lines. If x_start_at_zero is True, the x-axis starts at 0.\n",
    "    \"\"\"\n",
    "    # Increase left and bottom spine thickness\n",
    "    for spine in ['left', 'bottom']:\n",
    "        axis.spines[spine].set_linewidth(2)\n",
    "\n",
    "    # Remove top and right spine\n",
    "    for spine in ['top', 'right']:\n",
    "        axis.spines[spine].set_visible(False)\n",
    "\n",
    "    # Increase axis tick thickness\n",
    "    axis.tick_params(axis='both', width=2)\n",
    "\n",
    "    # Add horizontal grid below data markers\n",
    "    axis.yaxis.grid(True, linewidth=1)\n",
    "    axis.set_axisbelow(True)\n",
    "\n",
    "    # Set x-axis start at 0\n",
    "    if x_start_at_zero:\n",
    "        axis.set_xlim(left=0)\n",
    "\n",
    "    # Set titles/axis labels\n",
    "    if title:\n",
    "        axis.set_title(title)\n",
    "    if x_label:\n",
    "        axis.set_xlabel(x_label)\n",
    "    if y_label:\n",
    "        axis.set_ylabel(y_label)\n",
    "\n",
    "    # Toggle legend\n",
    "    if show_legend:\n",
    "        axis.legend()"
   ],
   "id": "34b15b4e1514ed22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Size",
   "id": "8645c90e30d1bc85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dfs = {'Combined Dataset': whole_df, 'Train/Eval Split': train_df, 'Test Split': test_df}\n",
    "\n",
    "names = list(dfs.keys())\n",
    "sizes = [len(df) for df in dfs.values()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(names, sizes, color=['tab:blue', 'tab:orange', 'tab:green'])\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "style_wrapper(ax, title='Size of Data Splits', y_label='Number of Rows')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "14ee609877ba0f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total = len(dfs['Combined Dataset'])\n",
    "percent_train = len(dfs['Train/Eval Split']) / total * 100\n",
    "percent_test = len(dfs['Test Split']) / total * 100\n",
    "\n",
    "# Plotting\n",
    "_, ax = plt.subplots()\n",
    "bars = ax.bar(['Train/Eval Split', 'Test Split'], [percent_train, percent_test],\n",
    "              color=['tab:blue', 'tab:orange'])\n",
    "\n",
    "# Add percentage labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "style_wrapper(ax, title='Composition of Dataset', y_label='Percentage (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7aaa3e38cca1f9ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The train/test split was performed on categories. From the raw dataset, 8 categories were randomly picked and extracted from the train dataset. This was performed in such a manner, that the resulting test dataset contains at least 1000 unique prompts. This way, the evaluation will be performed on previously unseen tasks, which should show how well the model generalizes.",
   "id": "882b75d454d3b131"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt Count",
   "id": "d793f2ac5d053169"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "names = list(dfs.keys())\n",
    "sizes = [df['prompt'].nunique() for df in dfs.values()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(names, sizes, color=['tab:blue', 'tab:orange', 'tab:green'])\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "style_wrapper(ax, title='Unique Prompts per Data Split', y_label='Number of Unique Prompts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "730f9f277d2af389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt Distribution",
   "id": "84948af0de6e2bce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_prompt_distribution(df_list, title=None):\n",
    "    \"\"\"\n",
    "    Shows a boxplot showing the distribution of unique chosen/rejected values per prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_counts = [\n",
    "        df['prompt'].value_counts().values for df in df_list\n",
    "    ]\n",
    "    labels = ['Train Split', 'Test Split']\n",
    "\n",
    "    _, plot_ax = plt.subplots()\n",
    "    plot_ax.boxplot(prompt_counts, tick_labels=labels)\n",
    "    style_wrapper(\n",
    "        plot_ax, title=title, x_label=None, y_label='Frequency'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "9516431355b3f485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_prompt_distribution([train_df, test_df], title='Unique Prompts per Data Split')",
   "id": "a67a6f4e7b54b569",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chosen/Rejected distributions",
   "id": "c59747521834d876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_unique_response_distribution(df_list, title=None):\n",
    "    \"\"\"\n",
    "    Shows a boxplot showing the distribution of unique chosen/rejected values per prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate unique responses per prompt\n",
    "    chosen_per_prompt = [\n",
    "        df.groupby('prompt')['chosen'].nunique() for df in df_list\n",
    "    ]\n",
    "    labels = ['Train Split', 'Test Split']\n",
    "\n",
    "    _, plot_ax = plt.subplots()\n",
    "    plot_ax.boxplot(chosen_per_prompt, tick_labels=labels)\n",
    "    style_wrapper(\n",
    "        plot_ax, title=title, y_label='Responses per Prompt'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "b56f8cfcda164aa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_unique_response_distribution([train_df, test_df], 'Unique Instances per Prompt per Data Split')",
   "id": "f0884acc020ef972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Top Categories by Sample Count",
   "id": "200401e25c07683c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def truncate_labels(labels, max_length=20):\n",
    "    # Truncate labels to a given max length\n",
    "    return [label if len(label) <= max_length else label[:max_length - 3] + '...' for label in labels]\n",
    "\n",
    "\n",
    "def plot_top_categories(df, top_n=15, title=None):\n",
    "    category_counts = df['category'].value_counts()\n",
    "    top_categories = category_counts.head(top_n)\n",
    "\n",
    "    truncated_labels = truncate_labels(top_categories.index.tolist())\n",
    "    _, plot_ax = plt.subplots()\n",
    "    plot_ax.bar(truncated_labels, top_categories.values, color='tab:blue')\n",
    "\n",
    "    style_wrapper(\n",
    "        plot_ax, title=title,\n",
    "        x_label='Category', y_label='Number of Samples',\n",
    "    )\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "18577df6c34d9195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_top_categories(dfs['Train/Eval Split'], title='Top Categories by Sample Count in Train Split')",
   "id": "9513a9b8f6dbb8f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_top_categories(dfs['Test Split'], title='Top Categories by Sample Count in Test Split')",
   "id": "1cd21bbae1f7450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Samples per Category",
   "id": "b930da908f11da7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_sample_count_boxplot(df_list, title=None):\n",
    "    category_counts = [\n",
    "        df['category'].value_counts().values for df in df_list\n",
    "    ]\n",
    "    labels = ['Train Split', 'Test Split']\n",
    "\n",
    "    _, plot_ax = plt.subplots()\n",
    "    plot_ax.boxplot(category_counts, tick_labels=labels)\n",
    "\n",
    "    style_wrapper(\n",
    "        plot_ax, title=title,\n",
    "        y_label='Number of Samples per Category',\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "f2047c2d910554ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sample_count_boxplot([train_df, test_df], title='Number of Samples per Category')",
   "id": "514716728a3858c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Average Number of Prompts per Category",
   "id": "5500010798e711af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_prompts_per_category(df_list, title=None):\n",
    "    grouped = [\n",
    "        df.groupby('category')['prompt'].nunique().values for df in df_list\n",
    "    ]\n",
    "    labels = ['Train Split', 'Test Split']\n",
    "    _, plot_ax = plt.subplots()\n",
    "    plot_ax.boxplot(grouped, tick_labels=labels)\n",
    "\n",
    "    style_wrapper(\n",
    "        plot_ax, title=title,\n",
    "        y_label='Number of Unique Prompts',\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "7074be6f9de68f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_prompts_per_category([train_df, test_df], title='Unique Prompts per Category')",
   "id": "c63329858f822f45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokens per Prompt/Chosen/Rejected",
   "id": "d4f057405a1e5c70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('unsloth/Qwen2.5-3B-Instruct')"
   ],
   "id": "40b8d8cc5eebe0d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for key in ['Train/Eval Split', 'Test Split']:\n",
    "    df = dfs[key]\n",
    "    for column in ['prompt', 'chosen', 'rejected']:\n",
    "        df[column + '_tokens'] = df[column].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))"
   ],
   "id": "4c3ce914f9fcd18c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_token_length_dist(df, title=None, clip_position=800):\n",
    "    # Most prompts have sizes below 1000; only a few outliers go up to 8000\n",
    "    df_filtered = df[(df['prompt_tokens'] <= clip_position) &\n",
    "                     (df['chosen_tokens'] <= clip_position) &\n",
    "                     (df['rejected_tokens'] <= clip_position)]\n",
    "\n",
    "    _, plot_ax = plt.subplots()\n",
    "    plot_ax.boxplot(\n",
    "        df_filtered[['prompt_tokens', 'chosen_tokens', 'rejected_tokens']],\n",
    "        tick_labels=['Prompt', 'Chosen', 'Rejected']\n",
    "    )\n",
    "\n",
    "    style_wrapper(\n",
    "        plot_ax, title=title,\n",
    "        y_label='Number of Prompts'\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d00db38fbdab0be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_token_length_dist(dfs['Train/Eval Split'], title='Tokens per Prompt in Train Split', clip_position=600)",
   "id": "4b02735833aee592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_token_length_dist(dfs['Test Split'], title='Tokens per Prompt in Test Split', clip_position=600)",
   "id": "7de5fc81e6b3e04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Worst Category Evaluation",
   "id": "9fedf24fc72584d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_worst_cats = [\n",
    "    'arithmetic', 'philosophy', 'social_iqa',\n",
    "    'elementary_math_qa', 'biology', 'economics'\n",
    "]\n",
    "\n",
    "train_worst_cats = [\n",
    "    'anachronisms', 'analogical_similarity', 'strange_stories', 'real_or_fake_text',\n",
    "    'discourse_marker_prediction', 'contextual_parametric_knowledge_conflicts',\n",
    "    'winowhy', 'swahili_english_proverbs', 'checkmate_in_one',\n",
    "    'fantasy_reasoning', 'question_selection'\n",
    "]"
   ],
   "id": "722349ed2b8aa8d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_best_cats = ['hindu_knowledge', 'hinglish_toxicity', 'intent_recognition', 'cause_and_effect', 'truthfulqa', 'language_identification']",
   "id": "b46f181cc12572ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_cat_count(df, category_list=None):\n",
    "    if category_list is not None:\n",
    "        df = df[df['category'].isin(category_list)]\n",
    "    return df['category'].value_counts()"
   ],
   "id": "1919854a8c84e7c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_cat_count(stripped_train_df, train_best_cats)",
   "id": "5f97a62801d64320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_cat_count(train_df, train_best_cats)",
   "id": "959ff4e0e118d193",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_cat_count(stripped_test_df)",
   "id": "956de70b0f0a02bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df['category'].value_counts()",
   "id": "7e8f4c3166cd9c62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def average_tokens_per_category(df, avg_tokenizer, category_list):\n",
    "    \"\"\"\n",
    "    Computes the average number of tokens per category using a Hugging Face tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        :param df: DataFrame with category and text columns.\n",
    "        :param avg_tokenizer: Hugging Face tokenizer.\n",
    "        :param category_list:\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Average token count per category (sorted descending).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df[df['category'].isin(category_list)]\n",
    "    token_columns = ['orig_prompt', 'chosen', 'rejected']\n",
    "    for token_column in token_columns:\n",
    "        df.loc[:, f'{token_column}_token_count'] = df[token_column].apply(\n",
    "            lambda x: len(avg_tokenizer.encode(x, add_special_tokens=False))\n",
    "        )\n",
    "    token_columns = [f'{col}_token_count' for col in token_columns]\n",
    "    return (\n",
    "        df.groupby('category')[token_columns]\n",
    "        .mean()\n",
    "        .sort_values(by=token_columns, ascending=False)\n",
    "    )"
   ],
   "id": "9513b49e4be3ba44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "cat_tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B')"
   ],
   "id": "27a458be773dfad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "average_tokens_per_category(train_df, cat_tokenizer, train_best_cats)",
   "id": "8cb15cfcfd157f4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df[train_df['category'] == 'misconceptions']",
   "id": "1d5752f7ef2332b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df[train_df['category'] == 'checkmate_in_one']",
   "id": "c0c3f9829fc0beab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1d2e0e3b763c2383",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
